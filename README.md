# ğŸ§  MemoryRAG â€” LlamaIndex Edition

A production-grade **Multi-PDF RAG pipeline** built with **LlamaIndex** that remembers your conversation history. Available as both a **Streamlit web app** and a **terminal CLI**.

> **Part 3 of the DeepRAG series.**
> [Part 1 â€” Built from scratch](https://github.com/nadeem123-ai/DeepRAG) | [Part 2 â€” LangChain](your-link) | Part 3 â€” LlamaIndex â† you are here

---

## âœ¨ What Makes This Special

Most RAG systems treat every question independently. **MemoryRAG remembers.**

```
You:  "What are his technical skills?"
AI:   "He knows Python, LlamaIndex, ChromaDB..."

You:  "Tell me more about the first one"   â† no context given!
AI:   "Python is used for..."              â† knows "first one" = Python âœ…
```

And it works across **multiple PDFs at once** â€” ask about your resume, then your report, then compare them. It knows which document each answer came from.

---

## ğŸš€ Features

- ğŸ§  **Conversation Memory** â€” remembers all previous Q&A in a session
- ğŸ“„ **Multi-PDF Support** â€” load a single file, multiple files, or an entire folder
- ğŸŒ **Streamlit Web UI** â€” beautiful dark-themed chat interface with source pills
- ğŸ’» **Terminal CLI** â€” classic interactive mode still available
- âœ‚ï¸ **Smart Chunking** â€” SentenceSplitter preserves natural sentence boundaries
- ğŸ”¢ **HuggingFace Embeddings** â€” `BAAI/bge-small-en-v1.5` (384-dim vectors)
- ğŸ—„ï¸ **Chroma DB** â€” persistent vector store, no re-embedding on restart
- ğŸ¤– **Dual LLM Support** â€” Ollama (local/free) or OpenAI (cloud)
- ğŸªŸ **Windows Compatible** â€” handles Chroma file-locking gracefully

---

## ğŸ—ï¸ Project Structure

```
MemoryRAG-LlamaIndex/
â”œâ”€â”€ docs/                    â† put your PDF files here
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py          â† package entry point
â”‚   â”œâ”€â”€ loader.py            â† multi-PDF loading (file / list / folder)
â”‚   â”œâ”€â”€ splitter.py          â† SentenceSplitter (chunk + overlap)
â”‚   â”œâ”€â”€ embedder.py          â† HuggingFaceEmbedding (BAAI/bge-small-en-v1.5)
â”‚   â”œâ”€â”€ vector_store.py      â† ChromaDB persistent store (Windows-safe)
â”‚   â”œâ”€â”€ llm.py               â† Ollama + OpenAI unified interface
â”‚   â””â”€â”€ pipeline.py          â† CondensePlusContextChatEngine + ChatMemoryBuffer
â”œâ”€â”€ app.py                   â† Streamlit web UI
â”œâ”€â”€ main.py                  â† terminal CLI
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env                     â† OpenAI API key (optional)
```

---

## âš™ï¸ Installation

### 1. Clone the repo
```bash
git clone https://github.com/YOUR_USERNAME/MemoryRAG-LlamaIndex.git
cd MemoryRAG-LlamaIndex
```

### 2. Create virtual environment
```bash
python -m venv venv

# Mac/Linux
source venv/bin/activate

# Windows
venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```
> âš ï¸ Takes 5â€“10 minutes â€” downloads PyTorch + sentence-transformers.

### 4. Install Ollama + pull a model
Download from **https://ollama.com**, then:
```bash
# Recommended for 4â€“8 GB RAM
ollama pull qwen2.5:1.5b

# Alternatives
ollama pull qwen2.5:0.5b   # lightest (1 GB RAM)
ollama pull phi3:mini       # best quality on low RAM
ollama pull mistral         # needs 8 GB RAM
```

### 5. (Optional) OpenAI setup
```bash
# Create .env file
echo OPENAI_API_KEY=sk-... > .env
```

---

## ğŸ–¥ï¸ Usage

### Web UI (Streamlit) â€” recommended
```bash
streamlit run app.py
```
Opens at **http://localhost:8501**

1. Select PDF source in the sidebar (folder or upload)
2. Choose your model
3. Click **Load & Initialize**
4. Start chatting!

### Terminal CLI
```bash
# Load entire docs/ folder
python main.py --pdf docs/

# Load specific files
python main.py --pdf docs/resume.pdf docs/report.pdf

# Use OpenAI
python main.py --pdf docs/ --provider openai --model gpt-4o-mini

# Skip demo, jump to chat
python main.py --pdf docs/ --model qwen2.5:1.5b --no-demo
```

### Terminal commands
```
clear    â†’ reset conversation memory
history  â†’ show all previous Q&A
exit     â†’ quit
```

---

## ğŸ” How It Works

```
Your Question
      â†“
CondensePlusContextChatEngine
      â†“
  [ChatMemoryBuffer] condenses question with history
      â†“
  [ChromaDB] finds top-k relevant nodes across all PDFs
      â†“
  [LLM] generates answer using nodes + history
      â†“
  [ChatMemoryBuffer] saves Q&A for next turn
      â†“
Answer + Source Pills [resume.pdf Â· p1]  [islamiyat.pdf Â· p3]
```

---

## ğŸ”„ LangChain â†’ LlamaIndex Mapping

| LangChain | LlamaIndex | Role |
|-----------|-----------|------|
| `PyPDFLoader` | `SimpleDirectoryReader` | PDF loading |
| `RecursiveCharacterTextSplitter` | `SentenceSplitter` | Chunking |
| `HuggingFaceEmbeddings` | `HuggingFaceEmbedding` | Embeddings |
| `Chroma` (LC wrapper) | `ChromaVectorStore` + `VectorStoreIndex` | Vector store |
| `OllamaLLM` / `ChatOpenAI` | `Ollama` / `OpenAI` | LLM |
| `ConversationBufferMemory` | `ChatMemoryBuffer` | Memory |
| `ConversationalRetrievalChain` | `CondensePlusContextChatEngine` | Chain / Engine |
| `source_documents` | `source_nodes` | Source tracking |

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| Framework | LlamaIndex 0.10.x |
| Web UI | Streamlit |
| Embeddings | HuggingFace â€” BAAI/bge-small-en-v1.5 |
| Vector DB | Chroma DB (persistent, Windows-safe) |
| Local LLM | Ollama â€” qwen2.5:1.5b / mistral / phi3 |
| Cloud LLM | OpenAI â€” GPT-4o / GPT-4o-mini |
| Memory | ChatMemoryBuffer (token-limited) |
| Engine | CondensePlusContextChatEngine |
| PDF | SimpleDirectoryReader |
| Language | Python 3.10+ |

---

## ğŸªŸ Windows Notes

ChromaDB holds file locks on Windows which causes `WinError 32` when re-initialising. This project handles it automatically:

- `app.py` releases the pipeline and calls `gc.collect()` before touching `chroma_db/`
- `vector_store.py` falls back to an in-memory `EphemeralClient` if the persistent store is locked
- No manual deletion of `chroma_db/` needed

---

## ğŸ’¡ Key Learnings

**LlamaIndex's `CondensePlusContextChatEngine` > LangChain's `ConversationalRetrievalChain`**
It separately condenses the question AND retrieves context before answering â€” better follow-up accuracy.

**`Settings` global is cleaner than passing objects everywhere**
One line `Settings.llm = llm` and every component downstream uses it automatically.

**Build from scratch first**
I built DeepRAG (Part 1) manually before using LangChain (Part 2) and LlamaIndex (Part 3). Every abstraction made sense because I had already implemented it myself.

---

## ğŸ“„ License

MIT License â€” free to use, modify, and distribute.

---

## ğŸ™‹ Author

**Muhammad Nadeem**
AI / ML Engineer Â· LlamaIndex Â· RAG Â· Generative AI Â· LLM Systems

â­ If you found this useful, please give it a star!